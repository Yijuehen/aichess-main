"""
é˜¶æ®µ2æµ‹è¯•è„šæœ¬ - æ™ºèƒ½ä»»åŠ¡åˆ†é…

æµ‹è¯•å†…å®¹:
1. è¿›ç¨‹æ³¨å†Œå’Œè¿½è¸ª
2. ä»»åŠ¡åˆ†é… (collect/train)
3. å¿ƒè·³æœºåˆ¶
4. GPUè¯„åˆ†å’Œæ¨è
5. çŠ¶æ€æ±‡æ€»
"""
import time
import os
import redis
import logging
from gpu_balance.process_tracker import ProcessTracker
from gpu_balance.task_scheduler import TaskScheduler
from gpu_balance.config import get_config
from gpu_balance.gpu_monitor import GPUMonitor


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s'
)
logger = logging.getLogger('test_task_scheduler')


def test_redis_connection():
    """æµ‹è¯•1: Redisè¿æ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•1: Redisè¿æ¥")
    print("=" * 60)

    try:
        config = get_config()
        r = redis.StrictRedis(
            host=config.redis_host,
            port=config.redis_port,
            db=config.redis_db,
            decode_responses=True
        )
        r.ping()
        print("âœ… Redisè¿æ¥æˆåŠŸ")
        return r
    except Exception as e:
        print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
        return None


def test_process_registration(tracker):
    """æµ‹è¯•2: è¿›ç¨‹æ³¨å†Œ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: è¿›ç¨‹æ³¨å†Œ")
    print("=" * 60)

    # æ¨¡æ‹Ÿæ³¨å†Œcollectè¿›ç¨‹
    pid = os.getpid()
    success = tracker.register_process(
        pid=pid,
        gpu_id=0,
        proc_type='collect',
        priority=5
    )

    if success:
        print(f"âœ… è¿›ç¨‹æ³¨å†ŒæˆåŠŸ: PID={pid}, GPU=0, ç±»å‹=collect")
    else:
        print(f"âŒ è¿›ç¨‹æ³¨å†Œå¤±è´¥")
        return False

    # éªŒè¯æ³¨å†Œä¿¡æ¯
    info = tracker.get_process_info(pid)
    if info:
        print(f"âœ… è¿›ç¨‹ä¿¡æ¯è·å–æˆåŠŸ:")
        print(f"   PID: {info.pid}")
        print(f"   GPU: {info.gpu_id}")
        print(f"   ç±»å‹: {info.proc_type}")
        print(f"   çŠ¶æ€: {info.status}")
        print(f"   ä¼˜å…ˆçº§: {info.priority}")
        return True
    else:
        print("âŒ è¿›ç¨‹ä¿¡æ¯è·å–å¤±è´¥")
        return False


def test_heartbeat(tracker):
    """æµ‹è¯•3: å¿ƒè·³æœºåˆ¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: å¿ƒè·³æœºåˆ¶")
    print("=" * 60)

    pid = os.getpid()

    # å‘é€å¿ƒè·³
    success = tracker.update_heartbeat(
        pid=pid,
        games_completed=10,
        status='running'
    )

    if success:
        print("âœ… å¿ƒè·³æ›´æ–°æˆåŠŸ")

        # è·å–æ›´æ–°åçš„è¿›ç¨‹ä¿¡æ¯
        info = tracker.get_process_info(pid)
        if info:
            heartbeat_age = info.heartbeat_age
            print(f"   è·ç¦»ä¸Šæ¬¡å¿ƒè·³: {heartbeat_age:.2f}ç§’")
            print(f"   å®Œæˆæ¸¸æˆæ•°: {info.games_completed}")
            return True
    else:
        print("âŒ å¿ƒè·³æ›´æ–°å¤±è´¥")
        return False


def test_gpu_allocation(scheduler):
    """æµ‹è¯•4: GPUåˆ†é…"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: GPUåˆ†é…")
    print("=" * 60)

    # æµ‹è¯•ä¸ºå•ä¸ªcollectä»»åŠ¡åˆ†é…GPU
    print("\n4.1: ä¸ºå•ä¸ªcollectä»»åŠ¡åˆ†é…GPU")
    gpu_id = scheduler.allocate_gpu('collect')
    if gpu_id is not None:
        print(f"âœ… åˆ†é…GPU: {gpu_id}")
    else:
        print("âš ï¸  æ— å¯ç”¨GPU")

    # æµ‹è¯•ä¸ºå¤šä¸ªtrainä»»åŠ¡åˆ†é…GPU
    print("\n4.2: ä¸º3ä¸ªtrainä»»åŠ¡åˆ†é…GPU")
    gpu_ids = scheduler.allocate_gpus('train', count=3)
    if gpu_ids:
        print(f"âœ… åˆ†é…GPU: {gpu_ids}")
    else:
        print("âš ï¸  æ— å¯ç”¨GPU")

    # æµ‹è¯•åˆ†é…æ‰€æœ‰å¯ç”¨GPU
    print("\n4.3: åˆ†é…æ‰€æœ‰å¯ç”¨GPU")
    all_gpus = scheduler.allocate_gpus('collect', count=-1)
    if all_gpus:
        print(f"âœ… å¯ç”¨GPU: {all_gpus}")
    else:
        print("âš ï¸  æ— å¯ç”¨GPU")

    return True


def test_gpu_scoring(scheduler):
    """æµ‹è¯•5: GPUè¯„åˆ†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5: GPUè¯„åˆ†")
    print("=" * 60)

    # è·å–å¯ç”¨GPUåˆ—è¡¨
    from gpu_balance.utils import get_available_gpus
    config = get_config()
    available = get_available_gpus(
        scheduler.redis_client,
        min_memory_mb=config.gpu_balancing['thresholds']['min_memory_mb'],
        max_utilization=config.gpu_balancing['thresholds']['max_utilization']
    )

    if not available:
        print("âš ï¸  æ— å¯ç”¨GPUè¿›è¡Œè¯„åˆ†")
        return True

    print(f"\nä¸º {len(available)} ä¸ªå¯ç”¨GPUè¯„åˆ†:")

    for gpu_id in available:
        score = scheduler.score_gpu(gpu_id, 'collect')
        print(f"\nGPU {gpu_id}:")
        print(f"  æ€»åˆ†: {score.score:.1f}/100")
        print(f"  åˆ©ç”¨ç‡: {score.utilization:.1f}%")
        print(f"  å¯ç”¨å†…å­˜: {score.memory_free_mb}MB")
        print(f"  è¿›ç¨‹æ•°: {score.num_processes}")
        print(f"  è¯„åˆ†è¯¦æƒ…:")
        for reason in score.reasons:
            print(f"    - {reason}")

    return True


def test_allocation_recommendation(scheduler):
    """æµ‹è¯•6: åˆ†é…æ¨è"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•6: åˆ†é…æ¨è")
    print("=" * 60)

    test_cases = [
        (1, 'collect'),
        (4, 'collect'),
        (8, 'collect'),
        (1, 'train'),
        (3, 'train'),
    ]

    for num_tasks, task_type in test_cases:
        print(f"\n{num_tasks}ä¸ª{task_type}ä»»åŠ¡:")
        rec = scheduler.recommend_allocation(task_type, num_tasks)
        print(f"  ç­–ç•¥: {rec['strategy']}")
        print(f"  GPU: {rec['gpu_ids']}")
        print(f"  åŸå› :")
        for reason in rec['reasons']:
            print(f"    - {reason}")

    return True


def test_status_summary(tracker, scheduler):
    """æµ‹è¯•7: çŠ¶æ€æ±‡æ€»"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•7: çŠ¶æ€æ±‡æ€»")
    print("=" * 60)

    # ä»process trackerè·å–æ±‡æ€»
    print("\n7.1: è¿›ç¨‹è¿½è¸ªå™¨çŠ¶æ€æ±‡æ€»")
    summary = tracker.get_status_summary()

    if summary:
        print(f"æ€»è¿›ç¨‹æ•°: {summary.get('total_processes', 0)}")
        print(f"è¿è¡Œä¸­: {summary.get('running', 0)}")
        print(f"å¡ä½: {summary.get('stuck', 0)}")
        print(f"æŒ‰GPUåˆ†å¸ƒ: {summary.get('by_gpu', {})}")
        print(f"æŒ‰ç±»å‹åˆ†å¸ƒ: {summary.get('by_type', {})}")

    # ä»task schedulerè·å–æ±‡æ€»
    print("\n7.2: ä»»åŠ¡è°ƒåº¦å™¨çŠ¶æ€æ±‡æ€»")
    status = scheduler.get_allocation_status()

    if status:
        print(f"æ€»è¿›ç¨‹æ•°: {status.get('total_processes', 0)}")
        print(f"æŒ‰ç±»å‹: {status.get('by_type', {})}")
        print(f"æŒ‰GPU: {status.get('by_gpu', {})}")
        print(f"å¯ç”¨GPU: {status.get('available_gpus', [])}")

    return True


def test_cleanup(tracker):
    """æµ‹è¯•8: æ¸…ç†æµ‹è¯•æ•°æ®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•8: æ¸…ç†æµ‹è¯•æ•°æ®")
    print("=" * 60)

    pid = os.getpid()
    success = tracker.unregister_process(pid)

    if success:
        print(f"âœ… æµ‹è¯•è¿›ç¨‹å·²æ³¨é”€: PID={pid}")
    else:
        print(f"âš ï¸  è¿›ç¨‹æ³¨é”€å¤±è´¥: PID={pid}")

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("é˜¶æ®µ2æµ‹è¯•: æ™ºèƒ½ä»»åŠ¡åˆ†é…")
    print("=" * 60)

    # æµ‹è¯•Redisè¿æ¥
    redis_client = test_redis_connection()
    if not redis_client:
        print("\nâŒ Redisæœªè¿è¡Œï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        print("\nå¯åŠ¨æ–¹æ³•:")
        print("  sudo systemctl start redis")
        print("  æˆ–")
        print("  redis-server --daemonize yes")
        return

    # åˆå§‹åŒ–
    config = get_config()
    tracker = ProcessTracker(redis_client=redis_client, config=config)
    scheduler = TaskScheduler(redis_client=redis_client, config=config)

    # å…ˆå¯åŠ¨GPUç›‘æ§ï¼ˆå¦‚æœæœªè¿è¡Œï¼‰
    print("\n" + "=" * 60)
    print("å¯åŠ¨GPUç›‘æ§...")
    print("=" * 60)

    monitor = GPUMonitor(redis_client=redis_client, config=config)
    metrics_dict = monitor.monitor_once()

    if metrics_dict:
        print(f"âœ… GPUç›‘æ§æˆåŠŸï¼Œæ£€æµ‹åˆ° {len(metrics_dict)} ä¸ªGPU")
        for gpu_id, metrics in metrics_dict.items():
            print(f"   GPU {gpu_id}: {metrics.name}")
            print(f"     åˆ©ç”¨ç‡: {metrics.utilization:.1f}%")
            print(f"     å†…å­˜: {metrics.memory_used_mb}MB / {metrics.memory_total_mb}MB")
    else:
        print("âš ï¸  GPUç›‘æ§å¤±è´¥ï¼Œéƒ¨åˆ†æµ‹è¯•å¯èƒ½æ— æ³•è¿è¡Œ")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("è¿›ç¨‹æ³¨å†Œ", lambda: test_process_registration(tracker)),
        ("å¿ƒè·³æœºåˆ¶", lambda: test_heartbeat(tracker)),
        ("GPUåˆ†é…", lambda: test_gpu_allocation(scheduler)),
        ("GPUè¯„åˆ†", lambda: test_gpu_scoring(scheduler)),
        ("åˆ†é…æ¨è", lambda: test_allocation_recommendation(scheduler)),
        ("çŠ¶æ€æ±‡æ€»", lambda: test_status_summary(tracker, scheduler)),
        ("æ¸…ç†æµ‹è¯•æ•°æ®", lambda: test_cleanup(tracker)),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
            time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿ
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))

    # æµ‹è¯•ç»“æœæ±‡æ€»
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status}: {test_name}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")

    print("\n" + "=" * 60)


if __name__ == '__main__':
    main()
